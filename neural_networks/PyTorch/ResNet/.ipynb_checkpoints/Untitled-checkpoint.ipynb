{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinate-daisy",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- [Clean, scalable and easy to use ResNet implementation in Pytorch, Francesco Saverio Zuppichini.](https://github.com/FrancescoSaverioZuppichini/ResNet)\n",
    "- [Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-cycling",
   "metadata": {},
   "source": [
    "__Redigir isso aqui em inglês (para praticar).__\n",
    "\n",
    "As operações de convolução aplicadas ao longo da rede devem manter a dimensionalidade do tensor de entrada, e a redução dessa dimensionalidade é controlada pelas operações de pooling (operação essa que tanto reduz o custo de processamento quanto tem benefícios quanto ao aprendizado do modelo).\n",
    "\n",
    "É possível determinar relações aritméticas simples entre a dimensão de saída da convolução e os seus parâmetros, em particular, utilizando o parâmetro unitário de stride (s) (quantidade de passos do kernel convolucional) e zero padding (p) (adição de zeros nas bordas do tensor), sendo $i$, $k$ e $o$ as dimensões de input, do kernel e de output, respectivamente, temos a seguinte relação:\n",
    "\n",
    "__Relação 1:__ Para quaisquer $i$, $k$, $p$ e $s = 1$, vale que $o = (i - k) + 2p + 1$.\n",
    "\n",
    "Portanto, supondo que $k = 2\\cdot n + 1$ para algum $n \\in \\mathbb{N}$, então podemos definir o parâmetro de padding como $p = k // 2 = n$, e conseguimos $o = (i - 2\\cdot n - 1) + 2 \\cdot [(2n + 1) // 2] + 1 = i$. Para simplificar a implementação, é possível implementar (classe __Conv2dAuto__) o padding automático como uma extensão da classe base de convolução bidimensional disponível no PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding = (self.kernel_size[0] // 2, self.kernel_size[1] // 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-surfing",
   "metadata": {},
   "source": [
    "__Bloco Residual__\n",
    "\n",
    "Explicar o funcionamento do bloco residual.\n",
    "\n",
    "Responder:\n",
    "\n",
    "1. Por que utilizar uma classe abstrata ResidualBlock?\n",
    "\n",
    "Explicar:\n",
    "\n",
    "1. \"property\"\n",
    "2. Expansion e Downsampling\n",
    "3. self.conv no ResNetResidualBlock\n",
    "4. Diferença entre ResidualBlock e ResNetResidualBlock\n",
    "5. Ordered Dict\n",
    "6. Por que usar uma convolução de 1 entrada no shortcut?\n",
    "7. Diferença entre ResidualBlock, ResNetResidualBlock, ResNetBasicBlock\n",
    "8. Pra que serve o BottleNeckBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "horizontal-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.blocks = nn.Identity()\n",
    "        self.shortcut = nn.Identity()\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        residual = self.shortcut(x) if self.should_apply_shortcut else x\n",
    "        x = self.blocks(x) + residual\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "def conv_bn(in_channels, out_channels, convolution, *args, **kwargs) -> Sequential:\n",
    "    return nn.Sequential(OrderedDict({\n",
    "        'conv': convolution(in_channels, out_channels, *args, **kwargs),\n",
    "        'bn': nn.BatchNorm2d(out_channels)\n",
    "    }))\n",
    "    \n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels: int, out_channels: int, expansion: int = 1, downsampling: int = 1, \n",
    "                 *args, **kwargs):\n",
    "        self.expansion = expansion\n",
    "        self.downsampling = downsampling\n",
    "        self.conv = partial(Conv2dAuto, kernel_size = 3, bias = False) # Really needed?\n",
    "        \n",
    "        self.shortcut = conv_bn(self.in_channels, self.expanded_channels, nn.Conv2d, \n",
    "                                kernel_size = 1, stride = self.downsampling, bias = False) if self.should_apply_shortcut else None\n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "    \n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, activation = nn.ReLU, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv = self.conv, bias = False, stride = self.downsampling),\n",
    "            activation(),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv = self.conv, bias = False),\n",
    "        )\n",
    "        \n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, activation = nn.ReLU, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion = 4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size = 1),\n",
    "            activation(),\n",
    "            conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size = 3, stride = self.downsampling),\n",
    "            activation(),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size = 1),\n",
    "        )\n",
    "        \n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, block = ResNetBasicBlock, n = 1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        downsmapling = 2 if in_channels != out_channels else 1\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels, out_channels, *args, **kwargs, downsampling = downsmapling),\n",
    "            *[block(out_channels * block.expansion, out_channels, downsampling = 1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-roller",
   "metadata": {},
   "source": [
    "__Estrutura do Texto__\n",
    "\n",
    "1. Explicação das componentes da rede.\n",
    "2. Explicação dos dois módulos principais da rede.\n",
    "3. Rede propriamente dita.\n",
    "\n",
    "Por que aumentar os atributos ao longo do encoder?\n",
    "\n",
    "Para que serve o gate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels = 3, blocks_sizes = [64, 128, 256, 512], depths = [2, 2, 2, 2],\n",
    "                 activation = nn.ReLU, block = ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ResNetLayer()\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
